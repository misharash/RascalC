{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair counting for Tutorial: Aperiodic Data and Jackknives\n",
    "\n",
    "This computes the pair counts and correlation functions for [the main tutorial notebook](tutorial.ipynb).\n",
    "If you already have the pair counting results saved, you do not need to run this notebook again unless you change the settings affecting the correlation function and/or pair counts.\n",
    "\n",
    "This part should be separate from the main notebook/script for two reasons:\n",
    "\n",
    "- If `pycorr` pair counting is called from Python with multi-threading (OpenMP) enabled, it may disable multi-threading (OpenMP) in all subsequent external computations, including `RascalC`, harming their performance.\n",
    "- Pair counts (usually) do not need to be recomputed before each `RascalC` run.\n",
    "\n",
    "We will have to repeat the data preparation steps from [the main tutorial notebook](tutorial.ipynb):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the catalogs\n",
    "\n",
    "First of all, we will take precautions against repeated warnings disappearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "galaxies_filename = \"mock_galaxy_DR12_CMASS_N_QPM_0001.rdzw\"\n",
    "randoms_filename = \"mock_random_DR12_CMASS_N_50x1.rdzw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the original files: text with RA, DEC, Z (redshift) and weight columns.\n",
    "This, and many of the next cells, may take about 10 seconds because of the large number of randoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "galaxies = Table(np.loadtxt(galaxies_filename, usecols = range(4)), names = [\"RA\", \"DEC\", \"Z\", \"WEIGHT\"]) # ignore the last column, not sure what it is\n",
    "randoms = Table(np.loadtxt(randoms_filename, usecols = range(4)), names = [\"RA\", \"DEC\", \"Z\", \"WEIGHT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the comoving distance within the fiducial (grid) cosmology. Here we use a utility function from the RascalC library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from RascalC.pre_process import comoving_distance_Mpch\n",
    "Omega_m = 0.29; Omega_k = 0; w_DE = -1 # density parameters of matter and curvature, and the equation-of-state parameter of dark energy\n",
    "galaxies[\"comov_dist\"] = comoving_distance_Mpch(galaxies[\"Z\"], Omega_m, Omega_k, w_DE)\n",
    "randoms[\"comov_dist\"] = comoving_distance_Mpch(randoms[\"Z\"], Omega_m, Omega_k, w_DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a utility function for position formatting that will be useful on several more occasions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rdd_positions(catalog: Table) -> tuple[np.ndarray[float]]: # utility function to format positions from a catalog\n",
    "    return (catalog[\"RA\"], catalog[\"DEC\"], catalog[\"comov_dist\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign jackknife regions to both galaxies and randoms.\n",
    "\n",
    "The `get_subsampler_xirunpc` function generates a $K$-means subsampler from `sklearn` under the rug through a `pycorr` interface, ensuring that it is run single-threaded.\n",
    "If you call it in your own way, beware that it might run multi-threaded if `OMP_` environment variables are set, and that is known to impede the performance of pair counting or the main `RascalC` covariance computation later (due to OpenMP limitations).\n",
    "\n",
    "$K$-means is nice in that it can generate a fixed number of regions of similar size with realistic survey geometry.\n",
    "(However, it is not without issues, e.g. it does not guarantee similar completeness patterns that may affect the shot-noise rescaling.)\n",
    "Previously, jackknife region numbers were assigned as `healpix` pixels (with number controlled by `NSIDE` variable), which is simpler, but less balanced and flexible.\n",
    "In cubic/rectangular boxes, box subsamplers from `pycorr` may be useful.\n",
    "If you have better ideas, feel free to use them and perhaps share with the code authors if they work out well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from RascalC.pre_process import get_subsampler_xirunpc\n",
    "n_jack = 60 # number of regions\n",
    "subsampler = get_subsampler_xirunpc(get_rdd_positions(galaxies), n_jack, position_type = \"rdd\") # \"rdd\" means RA, DEC in degrees and then distance (corresponding to pycorr)\n",
    "galaxies[\"JACK\"] = subsampler.label(get_rdd_positions(galaxies), position_type = \"rdd\")\n",
    "randoms[\"JACK\"] = subsampler.label(get_rdd_positions(randoms), position_type = \"rdd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a smaller subset of randoms to make pair counting and `RascalC` importance sampling more feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_randoms = 10 # how many times the number of galaxies should the number of randoms be; the total number of randoms is ≈50x the number of galaxies\n",
    "np.random.seed(42) # for reproducibility\n",
    "randoms_subset = randoms[np.random.choice(len(randoms), x_randoms * len(galaxies), replace = False, p = randoms[\"WEIGHT\"] / np.sum(randoms[\"WEIGHT\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair counts and correlation functions with [`pycorr`](https://github.com/cosmodesi/pycorr)\n",
    "\n",
    "In addition to the galaxy and randoms catalogs, `RascalC` requires the random counts and correlation functions.\n",
    "We use [`pycorr`](https://github.com/cosmodesi/pycorr) — a wrapper over the fast pair-counting engine `Corrfunc` that saves all the counts and allows to re-bin.\n",
    "So it can nicely do in one go what used to require several scripts in the legacy version of this tutorial.\n",
    "\n",
    "We remind you to make sure your current Python environment has [the custom version of `Corrfunc`](https://github.com/cosmodesi/Corrfunc) as suggested for [`pycorr`](https://github.com/cosmodesi/pycorr) (see also [the Installing dependencies section](#Installing-dependencies) in the beginning of this notebook).\n",
    "\n",
    "Pair counting requires the same catalogs as the main `RascalC` covariance computation.\n",
    "To minimize the repetitions and/or avoid splitting in this tutorial, we decided to incorporate the `pycorr` run here.\n",
    "This required some extra tricks to avoid the OpenMP (multi-threading) interference, even more so to make this run safely in a Jupyter notebook (and not just in a script).\n",
    "\n",
    "In a \"production\" run, you might want to set up the `pycorr` pair counting separately.\n",
    "We typically do so, noting that it can be performed on GPU while the main covariance computation with `RascalC` proper requires CPU only.\n",
    "But it is **very important** to reproduce the pre-processing (including jackknife assignment) for `RascalC` inputs in the same way it was done for pair counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we choose whether to use full randoms or a smaller subset for pair counting.\n",
    "The latter is faster but less precise (and may cause convergence problems later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randoms_for_counts = randoms\n",
    "# randoms_for_counts = randoms_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue by splitting the randoms into parts of roughly the same size as data.\n",
    "This gives high precision at fixed computing cost [(Keihänen et al 2019)](https://arxiv.org/abs/1905.01133)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting randoms into 10 parts\n"
     ]
    }
   ],
   "source": [
    "n_splits = int(np.rint(len(randoms_for_counts) / len(galaxies))) # the number of parts to split the randoms to\n",
    "print(f\"Splitting randoms into {n_splits} parts\")\n",
    "\n",
    "# split randoms into the desired number of parts randomly\n",
    "random_indices = np.arange(len(randoms_for_counts))\n",
    "np.random.seed(42) # for reproducibility\n",
    "np.random.shuffle(random_indices) # random shuffle in place\n",
    "random_parts = [randoms_for_counts[random_indices[i_random::n_splits]] for i_random in range(n_splits)] # quick way to produce parts of almost the same size\n",
    "\n",
    "# normalize the weights in each part — fluctuations in their sums may be a bit of a problem\n",
    "for i_random in range(n_splits): random_parts[i_random][\"WEIGHT\"] /= np.sum(random_parts[i_random][\"WEIGHT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for pair counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_threads = 10 # number of threads for pycorr computation, feel free to adjust according to your CPU\n",
    "\n",
    "split_above = 20 # Mpc/h. Below this, will use concatenated randoms. Above, will use split.\n",
    "s_max = 200 # maximal separation in Mpc/h\n",
    "n_mu = 200 # number of angular (µ) bins\n",
    "counts_filename = f\"allcounts_mock_galaxy_DR12_CMASS_N_QPM_0001_lin_njack{n_jack}_nran{n_splits}_split{split_above}.npy\" # filename to save counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to compute the counts, using split randoms at larger separations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycorr import TwoPointCorrelationFunction, setup_logging\n",
    "from tqdm import trange # nice progress bar\n",
    "\n",
    "mu_edges = np.linspace(-1, 1, n_mu + 1) # make uniform µ bins between -1 and 1, or twice less bins between 0 and 1 after wrapping (will be done within RascalC wrapper)\n",
    "s_edges_all = (np.arange(split_above + 1), np.arange(split_above, s_max + 1)) # 1 Mpc/h wide separation bins from 0 to s_max Mpc/h, separated to concatenated/split random regions. Can be rebinned to any bin width that divides split_above and s_max\n",
    "\n",
    "def run_pair_counts():\n",
    "    setup_logging()\n",
    "    results = []\n",
    "    # compute\n",
    "    for i_split_randoms, s_edges in enumerate(s_edges_all):\n",
    "        result = 0\n",
    "        D1D2 = None # to compute the data-data counts on the first go but not recompute then\n",
    "        for i_random in trange(n_splits if i_split_randoms else 1, desc=\"Computing counts with random part\"):\n",
    "            these_randoms = random_parts[i_random] if i_split_randoms else randoms_for_counts\n",
    "            tmp = TwoPointCorrelationFunction(mode = 'smu', edges = (s_edges, mu_edges),\n",
    "                                            data_positions1 = get_rdd_positions(galaxies), data_weights1 = galaxies[\"WEIGHT\"], data_samples1 = galaxies[\"JACK\"],\n",
    "                                            randoms_positions1 = get_rdd_positions(these_randoms), randoms_weights1 = these_randoms[\"WEIGHT\"], randoms_samples1 = these_randoms[\"JACK\"],\n",
    "                                            position_type = \"rdd\", engine = \"corrfunc\", D1D2 = D1D2, gpu = False, nthreads = n_threads)\n",
    "            # \"rdd\" means RA, DEC in degrees and then distance\n",
    "            D1D2 = tmp.D1D2 # once computed, becomes not None and will not be recomputed\n",
    "            result += tmp\n",
    "        results.append(result)\n",
    "    corr = results[0].concatenate_x(*results) # join the unsplit and split parts\n",
    "    corr.D1D2.attrs['nsplits'] = n_splits\n",
    "\n",
    "    corr.save(counts_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pair counts.\n",
    "The computation does take a while <!-- (about 10 minutes for me at NERSC login node; expect to see 1/10 progress in a few minutes) --> even with multi-threading (which I haven't managed to make work with `Corrfunc` in macOS yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing xi with random part: 100%|██████████| 10/10 [09:07<00:00, 54.73s/it]\n"
     ]
    }
   ],
   "source": [
    "run_pair_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell concluded successfully, you should be good to go back to [the main tutorial notebook](tutorial.ipynb) and load the saved pair counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
